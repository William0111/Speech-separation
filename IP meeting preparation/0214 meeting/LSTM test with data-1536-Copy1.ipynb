{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN test by data-10*512\n",
    "# 50000 steps with lr = 0.001\n",
    "# cell_size = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are better than\n",
    "best run: \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next_Train----------: step =  0\n",
      "train_cost:  0.036505251191556454 train_accuracy:  0.0029999999329447745\n",
      "test cost:  0.04413241958245635 test accuracy:  0.0029999999329447745\n",
      "Next_Train----------: step =  50\n",
      "train_cost:  0.021391115337610244 train_accuracy:  0.020999999716877937\n",
      "test cost:  0.029023744538426398 test accuracy:  0.013999999687075614\n",
      "Next_Train----------: step =  100\n",
      "train_cost:  0.018611581437289713 train_accuracy:  0.02999999951571226\n",
      "test cost:  0.026709588803350927 test accuracy:  0.012999999709427357\n",
      "Next_Train----------: step =  150\n",
      "train_cost:  0.017032187432050705 train_accuracy:  0.026999999582767487\n",
      "test cost:  0.026533837243914605 test accuracy:  0.017999999597668646\n",
      "Next_Train----------: step =  200\n",
      "train_cost:  0.016010009776800872 train_accuracy:  0.028999999351799487\n",
      "test cost:  0.025924617098644374 test accuracy:  0.014999999664723873\n",
      "Next_Train----------: step =  250\n",
      "train_cost:  0.014645625744014979 train_accuracy:  0.03299999926239252\n",
      "test cost:  0.026088322140276433 test accuracy:  0.01899999976158142\n",
      "Next_Train----------: step =  300\n",
      "train_cost:  0.014221033919602633 train_accuracy:  0.034999999403953555\n",
      "test cost:  0.026301239198073745 test accuracy:  0.021999999508261682\n",
      "Next_Train----------: step =  350\n",
      "train_cost:  0.013051870372146368 train_accuracy:  0.031999999284744264\n",
      "test cost:  0.026241265516728163 test accuracy:  0.01899999976158142\n",
      "Next_Train----------: step =  400\n",
      "train_cost:  0.012374457763507962 train_accuracy:  0.02799999937415123\n",
      "test cost:  0.025909705739468335 test accuracy:  0.019999999552965164\n",
      "Next_Train----------: step =  450\n",
      "train_cost:  0.011382197961211205 train_accuracy:  0.028999999351799487\n",
      "test cost:  0.025864716107025742 test accuracy:  0.012999999709427357\n",
      "Next_Train----------: step =  500\n",
      "train_cost:  0.011106063751503826 train_accuracy:  0.0419999998062849\n",
      "test cost:  0.025269714230671526 test accuracy:  0.01999999973922968\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Activation, Dense, Flatten,LSTM, TimeDistributed\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "TIME_STEPS = 3\n",
    "INPUT_SIZE = 512\n",
    "BATCH_SIZE = 50\n",
    "BATCH_INDEX = 0\n",
    "OUTPUT_SIZE = 1536\n",
    "CELL_SIZE = 800\n",
    "LR = 0.001\n",
    "\n",
    "\n",
    "           \n",
    "           \n",
    "# loading data\n",
    "X_train = np.loadtxt(\"0126_spec_train_1000.txt\")\n",
    "y_train = np.loadtxt(\"0126_mask_train_1000.txt\")\n",
    "\n",
    "X_test = np.loadtxt(\"0126_spec_test_1000.txt\")\n",
    "y_test = np.loadtxt(\"0126_mask_test_1000.txt\")\n",
    "\n",
    "# batch_size=y_test.shape[0]\n",
    "\n",
    "# data pre-processing\n",
    "X_train = X_train.reshape(-1, 3, 512)\n",
    "#y_train = y_train.reshape(-1, 3, 512)\n",
    "X_test = X_test.reshape(-1, 3, 512)\n",
    "#y_test = y_test.reshape(-1, 3, 512)\n",
    "\n",
    "#  build RNN model\n",
    "model = Sequential()\n",
    "\n",
    "# RNN cell\n",
    "\n",
    "model.add(LSTM(\n",
    "    units =1536,\n",
    "    batch_input_shape=( BATCH_SIZE, TIME_STEPS, INPUT_SIZE),\n",
    "    #input_dim=INPUT_SIZE,\n",
    "    #input_length=TIME_STEPS,\n",
    "    #output_dim=CELL_SIZE,\n",
    "    #return_sequences=True,\n",
    "    #stateful=True \n",
    "    unroll=True\n",
    "))\n",
    "\n",
    "# output layer\n",
    "#model.add(Flatten())\n",
    "model.add((Dense(OUTPUT_SIZE)))  #TimeDistributed\n",
    "#model.add(Activation('hard_sigmoid'))\n",
    "\n",
    "# # optimizer\n",
    "#\n",
    "adam = Adam(LR)\n",
    "\n",
    "#optimizer\n",
    "#rmsprop = RMSprop(lr=0.0001, rho=0.9, epsilon=1e-8, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# # training\n",
    "#\n",
    "\n",
    "for step in range(501):\n",
    "    # data shape = (batch_num, steps, inputs/outputs)\n",
    "    X_batch = X_train[BATCH_INDEX: BATCH_INDEX + BATCH_SIZE, :, :]\n",
    "    Y_batch = y_train[BATCH_INDEX: BATCH_INDEX + BATCH_SIZE, :]\n",
    "    cost = model.train_on_batch(X_batch, Y_batch)\n",
    "    BATCH_INDEX += BATCH_SIZE\n",
    "    BATCH_INDEX = 0 if BATCH_INDEX >= X_train.shape[0] else BATCH_INDEX\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print('Next_Train----------: step = ', step)\n",
    "        train_cost, train_accuracy = model.evaluate(X_train, y_train, batch_size=BATCH_SIZE, verbose=False)\n",
    "        print('train_cost: ', train_cost, 'train_accuracy: ', train_accuracy)\n",
    "        cost, accuracy = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=False)\n",
    "        print('test cost: ', cost, 'test accuracy: ', accuracy)\n",
    "        \n",
    "       \n",
    "\n",
    "# print('Train-------------------------')\n",
    "#\n",
    "# history = model.fit(X_train, y_train, validation_split=0.25, epochs=3100, shuffle=True, batch_size=75, verbose=1)\n",
    "#\n",
    "# # Plot training & validation accuracy values\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()\n",
    "#\n",
    "# # Plot training & validation loss values\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('Model loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()\n",
    "#\n",
    "# get_3rd_layer_output = K.function([model.layers[0].input],\n",
    "#                                   [model.layers[1].output])\n",
    "# layer_output1 = get_3rd_layer_output([X_train])[0]\n",
    "#\n",
    "# print(layer_output1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
